# Possible research projects and mini-projects

In no particular order:
* Investigate migration to nanochat
* Replicate anything from Feng23 or Baeumel25
* Get a robust addition model working and clearly characterize the points at which it fails
  - then, train it with chain of thought -- do we need more, less, or about the same resources?
  - then, give it scratch space to develop its own chain of thought -- Can we identify any resource savings from doing it this way?
* Figure out how to design certain GPTs by hand, then implement and test in PyTorch, e.g.:
  - copy an n-character input (for, say, n=3)
  - reverse an n-character input (for, say, n=3)
  - n-digit binary/ternary/decimal addition
  - markov chain and HMM
* For all models in the previous bullet point, experiment with training a model that is as similar as possible but created completely automatically from training.
* Mentor high school student
